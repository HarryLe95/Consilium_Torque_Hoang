{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e90166",
   "metadata": {},
   "source": [
    "# Alert Inference for Torque-related Failure Identification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import os\n",
    "# from typing import List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from scipy.signal import find_peaks\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "# import datetime\n",
    "# from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cb7a6",
   "metadata": {},
   "source": [
    "### 1. Function definitions\n",
    "Define all the functions to extract an alert based on a particular concept, as well as other functions, such as data loading, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_datetimes(event_data: pd.Series, merge_event_overlap: int = None):\n",
    "    \"\"\"\n",
    "    param data:\n",
    "    param merge_event_overlap:\n",
    "    return event_datetimes:\n",
    "    \"\"\"\n",
    "    inference_window_start_datetime = event_data.index[0]\n",
    "    inference_window_end_datetime = event_data.index[-1]\n",
    "\n",
    "    event_starts = event_data[event_data.diff() == 1].index.tolist()  # transition into an event\n",
    "    event_ends = event_data[event_data.diff() == -1].index.tolist()  # transition out of an event\n",
    "\n",
    "    if (len(event_starts) == 0) | (len(event_ends) == 0):\n",
    "        return None\n",
    "\n",
    "    # starts within event\n",
    "    if len(event_starts) < len(event_ends):\n",
    "        event_starts.insert(0, inference_window_start_datetime)\n",
    "\n",
    "    # ends within event\n",
    "    if len(event_starts) > len(event_ends):\n",
    "        event_ends.append(inference_window_end_datetime)\n",
    "\n",
    "    # starts and ends within event\n",
    "    if len(event_starts) == len(event_ends):\n",
    "        if event_starts[0] > event_ends[0]:\n",
    "            event_starts.insert(0, inference_window_start_datetime)\n",
    "            event_ends.append(inference_window_end_datetime)\n",
    "\n",
    "    event_datetimes = [(s, e) for s, e in zip(event_starts, event_ends)]\n",
    "\n",
    "    # merge events that are less than merge_event_overlap hours apart\n",
    "    # TODO: Recursively merge events\n",
    "#     if merge_event_overlap:\n",
    "#         event_datetimes_merged = []\n",
    "#         if len(event_datetimes) > 1:\n",
    "#             t_old = event_datetimes[0]\n",
    "#             for t in event_datetimes[1:]:\n",
    "#                 if (t_old[1] + datetime.timedelta(hours=merge_event_overlap)) >= t[0]:\n",
    "#                     t_old = ((min(t_old[0], t[0]), max(t_old[1], t[1])))\n",
    "#                 else:\n",
    "#                     event_datetimes_merged.append(t_old)\n",
    "#                     t_old = t\n",
    "\n",
    "#         event_datetimes = event_datetimes_merged\n",
    "\n",
    "    return event_datetimes if len(event_datetimes) > 0 else None\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "########These functions are for supporting get_std_spikes()##########\n",
    "\n",
    "# define a Limit to rolling function to 6 hours wehn taking 120 samples. \n",
    "# def roll_limiting_std(a):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     ix = a.index.max()\n",
    "#     a = a.loc[ix-pd.Timedelta('6h'):ix]\n",
    "#     return a.std()\n",
    "# # define a Limit to rolling function to 6 hours wehn taking 120 samples. \n",
    "# def roll_limiting_mean(a):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     ix = a.index.max()\n",
    "#     a = a.loc[ix-pd.Timedelta('6h'):ix]\n",
    "#     return a.mean()\n",
    "\n",
    "# def roll_limiting_sum(a):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     ix = a.index.max()\n",
    "#     a = a.loc[ix-pd.Timedelta('6h'):ix]\n",
    "#     return a.sum()\n",
    "\n",
    "# def roll_limiting_diff_sum(a):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     ix = a.index.max()\n",
    "#     a = a.loc[ix-pd.Timedelta('6h'):ix]\n",
    "#     return a.diff().sum()\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "# def get_std_spikes(well_data_df: pd.DataFrame,\n",
    "#                    prediction_name: str,\n",
    "#                    feature_name: str,\n",
    "#                    config_dict: dict) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     get_ramping_alerts finds ramps for a given well.\n",
    "\n",
    "#     :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "#     :param feature_name: the column name corresponding to features that is to be used in model inference.\n",
    "#     :param prediction_name: the column name corresponding to prediction in which model inference will be recorded.\n",
    "#     :param config_dict: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "#     :return: ...\n",
    "#     \"\"\"\n",
    "    \n",
    "#     feature_df=well_data_df.loc[:,[feature_name]]\n",
    "#     feature_df.loc[well_data_df[config_dict['operations_config']['operation_method']]==1,[feature_name]]=np.nan\n",
    "#     feature_df.loc[well_data_df[config_dict['flush_config']['operation_method']]==1,[feature_name]]=np.nan\n",
    "#     feature_df=feature_df[~feature_df[feature_name].isna()]\n",
    "#     rolled_tag=feature_df[feature_name].rolling(120)\n",
    "#     windowed_std=rolled_tag.apply(roll_limiting_std)\n",
    "#     windowed_mean=rolled_tag.apply(roll_limiting_mean)\n",
    "#     peaks=pd.DataFrame(index=windowed_std.index, columns=[feature_name])\n",
    "#     for time_ind in windowed_std.index:\n",
    "#         peaks.loc[time_ind,feature_name]=(windowed_std.loc[time_ind]-windowed_std.loc[windowed_std.loc[time_ind-pd.Timedelta(\"60min\"):time_ind].index.min()])\n",
    "#     peaks[feature_name]=peaks[feature_name]/windowed_mean#*np.sign(well_data_df[feat].rolling(120).apply(roll_limiting_diff_sum))\n",
    "#     peaks[peaks<0]=0\n",
    "#     peaks[rolled_tag.apply(roll_limiting_diff_sum)<0]=0 \n",
    "#     well_data_df[prediction_name]=0\n",
    "#     well_data_df.loc[peaks[peaks>config_dict['spike_diff_threshold']].dropna().index,[prediction_name]]=1\n",
    "\n",
    "#     return well_data_df\n",
    "\n",
    "\n",
    "def get_ramping_alerts(well_data_df: pd.DataFrame,\n",
    "                       feature_name: str,\n",
    "                       prediction_name: str,\n",
    "                       config_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get_ramping_alerts finds ramps for a given well.\n",
    "\n",
    "    :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "    :param feature_name: the column name corresponding to features that is to be used in model inference.\n",
    "    :param prediction_name: the column name corresponding to prediction in which model inference will be recorded.\n",
    "    :param config_dict: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "        \n",
    "    time_increments=well_data_df.index.to_series().resample('12h').max()    \n",
    "    torque_tag=well_data_df[feature_name]\n",
    "    torque_tag=torque_tag.interpolate(method='linear', axis=0)\n",
    "    differential=pd.DataFrame(index=time_increments,columns=['polynomial_diff'])\n",
    "    well_data_df[prediction_name]=0\n",
    "    for time in time_increments:\n",
    "        if time - pd.Timedelta(config_dict['polynomial_days']) > time_increments[0]:\n",
    "            x=torque_tag.loc[time-pd.Timedelta(config_dict['polynomial_days']):time]\n",
    "            if not well_data_df[config_dict['flush_config']['operation_method']].loc[time-pd.Timedelta(config_dict['polynomial_days']):time].any():\n",
    "                if not well_data_df[config_dict['operations_config']['operation_method']].loc[time-pd.Timedelta(config_dict['polynomial_days']):time].any():\n",
    "\n",
    "#                     if len(x)>9360:\n",
    "                    u_limit=x.mean()+(2.5*x.std())\n",
    "                    l_limit=x.mean()-(2.5*x.std())\n",
    "                    x[x > u_limit] = u_limit\n",
    "                    x[x < l_limit] = l_limit\n",
    "                    poly=np.polyfit(list(range(len(x))),x,deg=config_dict['polynomial_degree'])\n",
    "                    differential['polynomial_diff'].loc[time]=pd.DataFrame(np.polyval(poly, list(range(len(x))))).diff().mean()[0]\n",
    "    integral=differential['polynomial_diff'].rolling(config_dict['polynomial_days']).sum()\n",
    "\n",
    "    for time in differential.index:\n",
    "        if time - pd.Timedelta(config_dict['polynomial_days']) > well_data_df.index[0]:\n",
    "            if integral.loc[time]>0:\n",
    "                if integral.loc[time]*differential['polynomial_diff'].loc[time]*100000>config_dict['ramp_integral_threshold']:\n",
    "                    well_data_df.loc[time-pd.Timedelta('12hr'):time,[prediction_name]]=1\n",
    "    return well_data_df\n",
    "\n",
    "\n",
    "def get_torque_spike_decay_alert(well_data_df: pd.DataFrame,\n",
    "                                 feature_name: str,\n",
    "                                 prediction_name: str,\n",
    "                                 model_config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get_spike_decay finds the torque spike decay regions for a given well.\n",
    "\n",
    "    :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "    :param feature_name: the column name corresponding to feature that is to be used in model inference.\n",
    "    :param prediction_name: the column name corresponding to prediction in which model inference will be recorded.\n",
    "    :param model_config: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.load_model(model_config['model_path'], compile=False)\n",
    "\n",
    "    feature_data = well_data_df[feature_name].interpolate()\n",
    "    feature_data_idx = well_data_df[feature_name].index.values\n",
    "    window_size = model_config['window_size']\n",
    "    window_overlap = model_config['window_overlap']\n",
    "    binary_threshold = model_config['binary_threshold']\n",
    "\n",
    "    num_windows = int(np.floor(feature_data.shape[0] / (window_size / 2)))\n",
    "    window_starts = [int(i * window_size * (1 - window_overlap)) for i in range(num_windows)]\n",
    "    window_ends = [int((i * window_size * (1 - window_overlap)) + window_size) for i in range(num_windows)]\n",
    "    len_inference_timeframe = len(feature_data_idx)\n",
    "\n",
    "    edge_effect_width = int(window_size / (2 ** 4))\n",
    "\n",
    "    df = pd.DataFrame(index=feature_data_idx, columns=[prediction_name, f\"{prediction_name}_tmp\"])\n",
    "\n",
    "    for window_start, window_end in tqdm(zip(window_starts, window_ends)):\n",
    "        # get features (pad with zeros if not of length window_size)\n",
    "#         print(f\"window_start: {window_start} | window_end: {window_end}\")\n",
    "        fdata = np.expand_dims(feature_data[window_start:window_end], 0)\n",
    "        if fdata.shape[1] < window_size:\n",
    "            fdata = np.pad(fdata,\n",
    "                           ((0, 0),\n",
    "                            (0, window_size - fdata.shape[1])),\n",
    "                           # 'constant', constant_values=(0, 0)\n",
    "                           'edge'\n",
    "                           )\n",
    "\n",
    "        # get predictions of positive class (i.e. a present BDTA event)\n",
    "        prediction = model.predict(fdata)[0, :, 1]\n",
    "\n",
    "        # account for edge effects, except at start and end of inference timeframe\n",
    "        # start of inference timeframe\n",
    "        if window_start == 0:\n",
    "            s_idx = window_start\n",
    "            e_idx = window_end - 1 - edge_effect_width\n",
    "\n",
    "            s_pred_idx = 0\n",
    "            e_pred_idx = len(prediction) - edge_effect_width\n",
    "            # end of inference timeframe\n",
    "        elif (window_end > len_inference_timeframe):\n",
    "            s_idx = window_start + edge_effect_width\n",
    "            e_idx = len_inference_timeframe - 1\n",
    "\n",
    "            s_pred_idx = edge_effect_width\n",
    "            e_pred_idx = window_size - (window_end - len_inference_timeframe)\n",
    "\n",
    "        # within inference timeframe\n",
    "        else:\n",
    "            s_idx = window_start + edge_effect_width\n",
    "            e_idx = window_end - 1 - edge_effect_width\n",
    "\n",
    "            s_pred_idx = edge_effect_width\n",
    "            e_pred_idx = len(prediction) - edge_effect_width\n",
    "\n",
    "        # get datetime indices\n",
    "        dt_s_idx = df.index[s_idx]\n",
    "        dt_e_idx = df.index[e_idx]\n",
    "\n",
    "        # collate predictions in dataframe\n",
    "        df.loc[dt_s_idx:dt_e_idx, f\"{prediction_name}_tmp\"] = prediction[s_pred_idx:e_pred_idx]\n",
    "        df.loc[dt_s_idx:dt_e_idx, prediction_name] = df.loc[dt_s_idx:dt_e_idx].max(axis=1)\n",
    "\n",
    "    well_data_df[prediction_name] = (df[prediction_name].values > binary_threshold) * 1\n",
    "    \n",
    "    # filter datetimes\n",
    "    event_datetimes = get_event_datetimes(well_data_df[prediction_name],\n",
    "                                          merge_event_overlap=model_config['merged_event_overlap']\n",
    "                                         )\n",
    "    \n",
    "    if event_datetimes:\n",
    "        print(f\"Number of events detected: {len(event_datetimes)}\")\n",
    "        if (len(event_datetimes) > 0):\n",
    "            mask = [((pd.to_datetime(event_datetime[1]) - pd.to_datetime(event_datetime[0])) > pd.Timedelta(minutes=model_config['miminum_event_length'])) for event_datetime in event_datetimes]\n",
    "            filtered_event_datetimes = [x for x, y in zip(event_datetimes, mask) if y == True]\n",
    "\n",
    "            well_data_df[f\"{prediction_name}_filtered\"] = 0\n",
    "            for (start_datetime, end_datetime) in filtered_event_datetimes:\n",
    "                well_data_df[f\"{prediction_name}_filtered\"].loc[start_datetime:end_datetime] = 1\n",
    "\n",
    "            well_data_df[prediction_name] = well_data_df[f\"{prediction_name}_filtered\"]\n",
    "            well_data_df = well_data_df.drop(columns=[f\"{prediction_name}_filtered\"])\n",
    "\n",
    "    return well_data_df\n",
    "\n",
    "# def get_torque_spike_decay_alert_single_window(well_data_df: pd.DataFrame,\n",
    "#                                  feature_name: str,\n",
    "#                                  prediction_name: str,\n",
    "#                                  model_config: dict) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     get_spike_decay finds the torque spike decay regions for a given well.\n",
    "\n",
    "#     :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "#     :param feature_name: the column name corresponding to feature that is to be used in model inference.\n",
    "#     :param prediction_name: the column name corresponding to prediction in which model inference will be recorded.\n",
    "#     :param model_config: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "#     :return: ...\n",
    "#     \"\"\"\n",
    "\n",
    "#     model = keras.models.load_model(model_config['model_path'], compile=False)\n",
    "\n",
    "#     feature_data = well_data_df[feature_name].interpolate()\n",
    "#     feature_data_idx = well_data_df[feature_name].index.values\n",
    "#     window_size = model_config['window_size']\n",
    "#     window_overlap = model_config['window_overlap']\n",
    "#     binary_threshold = model_config['binary_threshold']\n",
    "\n",
    "#     df = pd.DataFrame(index=feature_data_idx, columns=[prediction_name, f\"{prediction_name}_tmp\"])\n",
    "    \n",
    "#     fdata = np.expand_dims(feature_data, 0)\n",
    "#     if fdata.shape[1] < window_size:\n",
    "#         fdata = np.pad(fdata,\n",
    "#                        ((0, 0),\n",
    "#                         (0, window_size - fdata.shape[1])),\n",
    "#                        # 'constant', constant_values=(0, 0)\n",
    "#                        'edge'\n",
    "#                        )\n",
    "            \n",
    "#     # get predictions of positive class (i.e. a present BDTA event)\n",
    "#     prediction = model.predict(fdata)[0, :, 1]\n",
    "#     df[prediction_name] = prediction\n",
    "\n",
    "#     well_data_df[prediction_name] = (df[prediction_name].values > binary_threshold) * 1\n",
    "    \n",
    "#     # filter datetimes\n",
    "#     event_datetimes = get_event_datetimes(well_data_df[prediction_name],\n",
    "#                                           merge_event_overlap=model_config['merged_event_overlap']\n",
    "#                                          )\n",
    "    \n",
    "#     if event_datetimes:\n",
    "#         print(f\"Number of events detected: {len(event_datetimes)}\")\n",
    "#         if (len(event_datetimes) > 0):\n",
    "#             mask = [((pd.to_datetime(event_datetime[1]) - pd.to_datetime(event_datetime[0])) > pd.Timedelta(minutes=model_config['miminum_event_length'])) for event_datetime in event_datetimes]\n",
    "#             filtered_event_datetimes = [x for x, y in zip(event_datetimes, mask) if y == True]\n",
    "\n",
    "#             well_data_df[f\"{prediction_name}_filtered\"] = 0\n",
    "#             for (start_datetime, end_datetime) in filtered_event_datetimes:\n",
    "#                 well_data_df[f\"{prediction_name}_filtered\"].loc[start_datetime:end_datetime] = 1\n",
    "\n",
    "#             well_data_df[prediction_name] = well_data_df[f\"{prediction_name}_filtered\"]\n",
    "#             well_data_df = well_data_df.drop(columns=[f\"{prediction_name}_filtered\"])\n",
    "\n",
    "#     return well_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_disk(data_definition_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw data from CSV file\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    well_data_df = pd.read_csv(data_definition_dict['well_raw_data_path'])\n",
    "    \n",
    "    if 'Unnamed: 0' in well_data_df.columns:\n",
    "        well_data_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "    well_data_df['TS'] = pd.to_datetime(well_data_df['TS'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    well_data_df.set_index(['TS'], inplace=True, drop=True)\n",
    "    \n",
    "    well_data_df = well_data_df.loc[(well_data_df.index > data_definition_dict['start_date']) & (well_data_df.index < data_definition_dict['end_date'])]\n",
    "\n",
    "#     print(f\"Well data loaded for {data_definition_dict['well_id']} for date range {data_definition_dict['start_date']} - {data_definition_dict['end_date']}\")\n",
    "\n",
    "    return well_data_df\n",
    "\n",
    "\n",
    "def which_feature(well_data_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Decide which feature to use.\n",
    "    \"\"\"\n",
    "\n",
    "    vals=[]\n",
    "    for t in feature_cols:\n",
    "#         print(t,well_data_df[t].count())\n",
    "        vals.append(well_data_df[t].count())\n",
    "    return feature_cols[np.argmax(vals)]\n",
    "\n",
    "\n",
    "def interpolate(well_data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return well_data_df.interpolate(method='linear', axis=0)\n",
    "\n",
    "\n",
    "def resample_to_1min(well_data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    well_data_df=well_data_df.groupby(['TS']).mean()\n",
    "    return well_data_df.asfreq('T', method='nearest').resample('1min').mean()\n",
    "#     well_data_df=well_data_df.reset_index()\n",
    "#     well_data_df['TS']=well_data_df['TS'].dt.round('min')\n",
    "#     return well_data_df.groupby(['TS']).mean()\n",
    "\n",
    "def load_labels_from_disk(label_definition_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load labels from CSV file\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    label_df = pd.read_csv(label_definition_dict['label_data_path'])\n",
    "    \n",
    "    label_df = label_df[label_df['WellCD'] == label_definition_dict['well_id']]\n",
    "\n",
    "    label_df['Event Date'] = pd.to_datetime(label_df['Event Date'], format=\"%d/%m/%Y\")\n",
    "\n",
    "    label_df.dropna(subset=['Event Date'], inplace=True)\n",
    "    \n",
    "    label_df = label_df[(label_df['Event Date'] > label_definition_dict['start_date']) & (label_df['Event Date'] < label_definition_dict['end_date'])]\n",
    "    \n",
    "    label_df.set_index('Event Date', inplace=True)\n",
    "\n",
    "    print(f\"Label data loaded for {label_definition_dict['well_id']} for date range {label_definition_dict['start_date']} - {label_definition_dict['end_date']}\")\n",
    "\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operation_status(well_data_df: pd.DataFrame,\n",
    "                         feature_name: str,\n",
    "                         config_dict: dict):\n",
    "    \"\"\"\n",
    "    get_off_times gets...\n",
    "\n",
    "    :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "    :param config_dict: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "\n",
    "    off_trace = pd.DataFrame(index=well_data_df.index, columns=[config_dict['operation_method']])\n",
    "    off_trace[config_dict['operation_method']] = 0\n",
    "    torque_well_data_df = well_data_df[feature_name].interpolate(method='linear', axis=0)\n",
    "    off_times = torque_well_data_df[torque_well_data_df < config_dict['off_threshold']].index\n",
    "    prev_point = well_data_df.index[0] - pd.Timedelta('6H')\n",
    "    for point in off_times:\n",
    "        if point > (prev_point + pd.Timedelta('5H')):\n",
    "            off_trace[config_dict['operation_method']].loc[point - pd.Timedelta('62min'):point + pd.Timedelta('5H')] = 1\n",
    "            prev_point = point\n",
    "    well_data_df[config_dict['operation_method']] = off_trace\n",
    "\n",
    "    return well_data_df\n",
    "\n",
    "\n",
    "def get_flush_status(well_data_df: pd.DataFrame,\n",
    "                     feature_name: str,\n",
    "                     config_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get_flush_status gets...\n",
    "\n",
    "    :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "    :param config_dict: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "\n",
    "    flush_trace = pd.DataFrame(index=well_data_df.index, columns=[config_dict['operation_method']])\n",
    "    flush_trace[config_dict['operation_method']] = 0\n",
    "    speed_well_data_df = well_data_df[feature_name].interpolate(method='linear', axis=0)\n",
    "    flush_diff = speed_well_data_df[speed_well_data_df.diff(1).abs().rolling('2H').sum() > config_dict['flush_diff_threshold']]\n",
    "    flush_std = speed_well_data_df[speed_well_data_df.rolling('2H').std() > config_dict['flush_std_threshold']]\n",
    "    exlude_ind = flush_diff.index.intersection(flush_std.index)\n",
    "    prev_point = well_data_df.index[0] - pd.Timedelta('6H')\n",
    "    for point in exlude_ind:\n",
    "        if point > (prev_point + pd.Timedelta('5H')):\n",
    "            flush_trace[config_dict['operation_method']].loc[point - pd.Timedelta('180min'):point + pd.Timedelta('12H')] = 1\n",
    "            prev_point = point\n",
    "    well_data_df[config_dict['operation_method']] = flush_trace\n",
    "\n",
    "    return well_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_alert(well_data_df: pd.DataFrame, alert_names: list, method: str = 'all') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'all':\n",
    "        well_data_df['combined_alert'] = (well_data_df[alert_names] == 1).all(1).astype(int)\n",
    "    elif method == 'any':\n",
    "        well_data_df['combined_alert'] = (well_data_df[alert_names] == 1).any(1).astype(int)\n",
    "    else:\n",
    "        well_data_df['combined_alert'] = 0\n",
    "\n",
    "    return well_data_df\n",
    "\n",
    "\n",
    "def get_static_plot_for_single_alert(well_id: str,\n",
    "                                     well_data_df: pd.DataFrame,\n",
    "                                     features_to_plot: list,\n",
    "                                     alert_names: str,\n",
    "                                     label_data_df: pd.DataFrame = None,\n",
    "                                     plot_type: str = 'scatter',\n",
    "                                     ylim: list = [0, 1000],\n",
    "                                     save_fig_fname: str = None\n",
    "                                    ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(50,8))\n",
    "    \n",
    "    for feature in features_to_plot:\n",
    "        if feature in well_data_df.columns:\n",
    "            if plot_type == 'scatter':\n",
    "                plt.scatter(well_data_df.index, well_data_df[feature])\n",
    "            if plot_type == 'line':\n",
    "                tmp_plot_df = well_data_df.dropna(subset=[feature])\n",
    "                plt.plot(tmp_plot_df.index, tmp_plot_df[feature])\n",
    "    for alert in alert_names:\n",
    "        plt.fill_between(x=well_data_df.index,\n",
    "                         y1=ylim[0], #0,\n",
    "                         y2=ylim[1], #int(well_data_df[features_to_plot[0]].dropna().max()*1.1),\n",
    "                         where=well_data_df[alert],\n",
    "#                          color='orange',\n",
    "                         alpha=0.5,\n",
    "                        )\n",
    "\n",
    "    if not label_data_df.empty:\n",
    "        for j, (idx, row) in enumerate(label_data_df.iterrows()):\n",
    "            plt.vlines(x=idx,\n",
    "                       ymin=ylim[0], #0,\n",
    "                       ymax=ylim[1], #int(well_data_df[features_to_plot[0]].dropna().max()*1.1),\n",
    "                       color='r'\n",
    "                      )\n",
    "            plt.text(x=idx,\n",
    "                     y=int(ylim[1] * (0.6 + (0.01 * j))), #int(well_data_df[features_to_plot[0]].dropna().max()),\n",
    "                     s=f\"{row['Event']} - {row['Failure Mode']} - {row['Flush_Comment']}\",\n",
    "#                      rotation=90,\n",
    "#                      verticalalignment='center'\n",
    "                    )\n",
    "\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "    plt.legend(features_to_plot + alert_names)#[f\"ALERT: {alert_names}\"])\n",
    "    plt.title(f\"{well_id}\")\n",
    "    \n",
    "    plt.ylim(ylim)\n",
    "#     plt.show()\n",
    "    \n",
    "    if save_fig_fname:\n",
    "        plt.savefig(save_fig_fname,\n",
    "#                     *, dpi='figure', format=None, metadata=None,\n",
    "#                     bbox_inches=None, pad_inches=0.1,\n",
    "#                     facecolor='auto', edgecolor='auto',\n",
    "#                     backend=None, **kwargs\n",
    "                   )\n",
    "        print(f\"Figure saved to: {save_fig_fname}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf751a4",
   "metadata": {},
   "source": [
    "# Generate the priority / Alert table for a given date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alert_monitor_generation_all_wells(date: str,\n",
    "                                       time_window: str,\n",
    "                                       well_list: list,\n",
    "                                      plot: bool,\n",
    "                                     torque_names: list,\n",
    "                                      completion_turndown_df: pd.DataFrame,\n",
    "                                      label_data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    start_date=pd.to_datetime(date)-pd.Timedelta('60d')\n",
    "    end_date=pd.to_datetime(date)#+pd.Timedelta('26d')\n",
    "    day_index=pd.date_range(start_date,end_date, freq='1h')\n",
    "    all_well_spike_alert_df=pd.DataFrame(columns=well_list,index=day_index)\n",
    "    all_well_ramp_alert_df=pd.DataFrame(columns=well_list,index=day_index)\n",
    "    all_well_total_alert_df=pd.DataFrame(columns=well_list,index=day_index)\n",
    "    todays_results_df=pd.DataFrame(index=well_list,columns=[\n",
    "                                                            f'Spiking Present (past {time_window}ay/s)',\n",
    "                                                            f'Ramping Present (past {time_window}ay/s)',\n",
    "                                                            'Spiking Present (past month)',\n",
    "                                                            'Ramping Present (past month)',\n",
    "                                             \n",
    "                                                                f'Spike Alert Percent Active Time in Past {time_window}ay/s',\n",
    "                                                                'Spike Alert Percent Active Time in Past Week',\n",
    "                                                                'Spike Alert Percent Active Time in Past 30 Days',\n",
    "                                                                'Spike Alert Percent Active Time in Past 60 Days',\n",
    "                                                                f'Ramp Alert Percent Active Time in Past {time_window}ay/s',\n",
    "                                                                'Ramp Alert Percent Active Time in Past Week',\n",
    "                                                                'Ramp Alert Percent Active Time in Past 30 Days',\n",
    "                                                                'Ramp Alert Percent Active Time in Past 60 Days',\n",
    "                                                               'Average gas flow (60 Days)',\n",
    "                                                               'TD status',\n",
    "                                                               'Design',\n",
    "                                                               'Pump age',\n",
    "                                                               'Days since last flush',\n",
    "                                                                'Days since last failure',\n",
    "                                                                f'Total Alert on minutes in Past {time_window}ay/s',\n",
    "                                                               'Total Alert on minutes in Past Week',\n",
    "                                                               'Total Alert on minutes in Past 30 Days',\n",
    "                                                               'Total Alert on minutes in Past 60 Days',\n",
    "                                                                f'Spike Alert on minutes in Past {time_window}ay/s',\n",
    "                                                                'Spike Alert on minutes in Past Week',\n",
    "                                                                'Spike Alert on minutes in Past 30 Days',\n",
    "                                                                'Spike Alert on minutes in Past 60 Days',\n",
    "                                                                f'Ramp Alert on minutes in Past {time_window}ay/s',\n",
    "                                                                'Ramp Alert on minutes in Past Week',\n",
    "                                                                'Ramp Alert on minutes in Past 30 Days',\n",
    "                                                                'Ramp Alert on minutes in Past 60 Days',])\n",
    "    \n",
    "\n",
    "    speed_names = ['SPEED_MOTOR', 'SPEED_ROD']\n",
    "                    \n",
    "    for well_id in well_list:\n",
    "        #peak_label_df is to be replaced by performing inference for the well here\n",
    "#         peak_label_df=pd.read_pickle(f'/home/ec2-user/SageMaker/efs/labels/segnet_inference/BRETT_Multi_Well_Model_v2/{well_id}.pkl')\n",
    "        print(well_id)\n",
    "        data_definition_dict = {\n",
    "            'well_id': well_id,\n",
    "            'well_raw_data_path': Path(f\"/home/ec2-user/SageMaker/efs/data/october_data_extract/tag_{well_id}.csv\"),\n",
    "            'start_date': pd.to_datetime(start_date, format=\"%Y-%m-%d\"),\n",
    "            'end_date': pd.to_datetime(end_date, format=\"%Y-%m-%d\")\n",
    "        }\n",
    "        well_data_df = load_data_from_disk(data_definition_dict)\n",
    "        if len(well_data_df)==0:\n",
    "            print(f'EMPTY DATA FOR WELL {well_id}')\n",
    "            pass\n",
    "        else:\n",
    "            # resample data\n",
    "            well_data_df = resample_to_1min(well_data_df)\n",
    "\n",
    "            torque_col = which_feature(well_data_df, torque_names)\n",
    "#             print(f\"Torque feature to be used: {torque_col}\")\n",
    "\n",
    "            speed_col = \"SPEED_ROD\" #which_feature(well_data_df, speed_names) #SPEED ROD ALWAYS\n",
    "#             print(f\"Speed feature to be used: {speed_col}\")\n",
    "            ### PREPROCESSING\n",
    "#             preprocessing_methods = [\n",
    "#             #     'high_pass_filtering'\n",
    "#             ]\n",
    "\n",
    "            ###---------------------------------------\n",
    "            ### PREPROCESSING - HIGH PASS FILTERING\n",
    "            ###---------------------------------------\n",
    "\n",
    "#             model_config = {\n",
    "#                 'preprocessing_method': 'high_pass_filtered_data',\n",
    "#                 'off_threshold': 10\n",
    "#             }\n",
    "\n",
    "\n",
    "#             if model_config['preprocessing_method'] in preprocessing_methods:\n",
    "#                 print(f\"Generating {model_config['preprocessing_method']}\")\n",
    "#                 well_data_df = get_high_pass_filtered_data(well_data_df,\n",
    "#                                                            feature_name=torque_col,\n",
    "#                                                            config_dict=model_config)\n",
    "\n",
    "#                 print(\"Complete\")\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"Did not generate {model_config['preprocessing_method']}\")\n",
    "\n",
    "#             ### OPERATION STATUS\n",
    "            operation_methods = [\n",
    "                'operation_status',\n",
    "                'flush_status',\n",
    "            ]\n",
    "\n",
    "            ###---------------------------------------\n",
    "            ### WELL OPERATION STATUS DETECTION - ON / OFF IDENTIFICATION\n",
    "            ###---------------------------------------\n",
    "\n",
    "            operations_config = {\n",
    "                'operation_method': 'operation_status',\n",
    "                'off_threshold': 10\n",
    "            }\n",
    "\n",
    "\n",
    "            if operations_config['operation_method'] in operation_methods:\n",
    "#                 print(f\"Generating {operations_config['operation_method']}\")\n",
    "                well_data_df = get_operation_status(well_data_df,\n",
    "                                                    feature_name=torque_col,\n",
    "                                                    config_dict=operations_config)\n",
    "\n",
    "\n",
    "            ###---------------------------------------\n",
    "            ### WELL FLUSH STATUS DETECTION\n",
    "            ###---------------------------------------\n",
    "\n",
    "            flush_config = {\n",
    "                'operation_method': 'flush_status',\n",
    "                'flush_diff_threshold': 4.9,\n",
    "                'flush_std_threshold': 4,\n",
    "            }\n",
    "\n",
    "            if flush_config['operation_method'] in operation_methods:\n",
    "#                 print(f\"Generating {flush_config['operation_method']}\")\n",
    "                well_data_df = get_flush_status(well_data_df,\n",
    "                                                feature_name=speed_col,\n",
    "                                                config_dict=flush_config)\n",
    "\n",
    "\n",
    "            ### Run inference for selected techniques on the loaded well data\n",
    "\n",
    "            # this list will be populated with each technique used for the alert creation\n",
    "            inference_methods = [\n",
    "                'torque_spike_decay_alert',\n",
    "                'ramping_alert',\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ###---------------------------------------\n",
    "            ### RAMPING ALERT\n",
    "            ###---------------------------------------\n",
    "\n",
    "            model_config = {\n",
    "                'inference_method': 'ramping_alert',\n",
    "                'polynomial_degree': 4,\n",
    "                'polynomial_days': '7d',\n",
    "                'ramp_integral_threshold': 0.7,\n",
    "                'operations_config': operations_config,\n",
    "                'flush_config': flush_config,\n",
    "            }\n",
    "\n",
    "            if model_config['inference_method'] in inference_methods:\n",
    "#                 print(f\"Generating {model_config['inference_method']}\")\n",
    "                well_data_df = get_ramping_alerts(well_data_df,\n",
    "                                                  feature_name=torque_col,\n",
    "                                                  prediction_name=model_config['inference_method'],\n",
    "                                                  config_dict=model_config)\n",
    "#                 print(\"Complete\")\n",
    "\n",
    "\n",
    "\n",
    "            ###---------------------------------------\n",
    "            ### TORQUE SPIKE DECAY DETECTION\n",
    "            ###---------------------------------------\n",
    "\n",
    "            model_config = {\n",
    "                'inference_method': 'torque_spike_decay_alert',\n",
    "                'model_path': Path(\"/home/ec2-user/SageMaker/efs/models/BRETT_Multi_Well_Model_v2.h5\"),\n",
    "                'window_size': 2 ** 14,\n",
    "                'window_overlap': 0.5,\n",
    "                'binary_threshold': 0.5,\n",
    "                'merged_event_overlap': 10,\n",
    "                'miminum_event_length': 0,\n",
    "            }\n",
    "\n",
    "            well_data_df[f\"{torque_col}_roll_avg\"] = well_data_df[torque_col].interpolate().rolling(120).mean()\n",
    "\n",
    "            if model_config['inference_method'] in inference_methods:\n",
    "#                 print(f\"Generating {model_config['inference_method']}\")\n",
    "                well_data_df = get_torque_spike_decay_alert(well_data_df,\n",
    "                                                            feature_name=f\"{torque_col}\",\n",
    "                                                            prediction_name=model_config['inference_method'],\n",
    "                                                            model_config=model_config)\n",
    "\n",
    "            well_data_df['operation_status'] = (well_data_df['operation_status'] == 0) * 1\n",
    "            well_data_df['flush_status'] = (well_data_df['flush_status'] == 0) * 1\n",
    "            well_data_df = get_combined_alert(well_data_df, alert_names=['torque_spike_decay_alert', 'flush_status','operation_status'])\n",
    "            peak_label_df=well_data_df.loc[:,['combined_alert']].resample('1h').sum()\n",
    "            ramp_label_df=well_data_df.loc[:,['ramping_alert']].resample('1h').sum()\n",
    "            total_alert_df=pd.DataFrame(index=peak_label_df.index)\n",
    "            total_alert_df['total_alert']=well_data_df.loc[:,['combined_alert']].resample('1h').sum().values+well_data_df.loc[:,['ramping_alert']].resample('1h').sum().values\n",
    "            all_well_spike_alert_df[well_id]=peak_label_df.reindex(index=all_well_spike_alert_df.index, method = 'nearest')\n",
    "            all_well_ramp_alert_df[well_id]=ramp_label_df.reindex(index=all_well_ramp_alert_df.index, method = 'nearest')\n",
    "            all_well_total_alert_df[well_id]=total_alert_df.reindex(index=all_well_spike_alert_df.index, method = 'nearest')\n",
    "\n",
    "            if all_well_total_alert_df[well_id].loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum()>0:\n",
    "                if  peak_label_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]>0:\n",
    "                    todays_results_df.loc[well_id,[f'Spiking Present (past {time_window}ay/s)']]='True'\n",
    "                else:\n",
    "                    todays_results_df.loc[well_id,[f'Spiking Present (past {time_window}ay/s)']]='False'\n",
    "                if  ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]>0:\n",
    "                    todays_results_df.loc[well_id,[f'Ramping Present (past {time_window}ay/s)']]='True'\n",
    "                else:\n",
    "                    todays_results_df.loc[well_id,[f'Ramping Present (past {time_window}ay/s)']]='False' \n",
    "                    \n",
    "                    \n",
    "                if  peak_label_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]>0:\n",
    "                    todays_results_df.loc[well_id,['Spiking Present (past month)']]='True'\n",
    "                else:\n",
    "                    todays_results_df.loc[well_id,['Spiking Present (past month)']]='False'\n",
    "                if  ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]>0:\n",
    "                    todays_results_df.loc[well_id,['Ramping Present (past month)']]='True'\n",
    "                else:\n",
    "                    todays_results_df.loc[well_id,['Ramping Present (past month)']]='False' \n",
    "                \n",
    "\n",
    "                todays_results_df.loc[well_id,[f'Total Alert on minutes in Past {time_window}ay/s']] = total_alert_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]#+ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Total Alert on minutes in Past Week']] = total_alert_df.loc[pd.to_datetime(date)-pd.Timedelta('7d'):pd.to_datetime(date)].sum().values[0]#+ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta('7d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Total Alert on minutes in Past 30 Days']] = total_alert_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]#+ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Total Alert on minutes in Past 60 Days']] = total_alert_df.loc[start_date:pd.to_datetime(date)].sum().values[0]#+ramp_label_df.loc[start_date:pd.to_datetime(date)].sum().values[0]\n",
    "\n",
    "                todays_results_df.loc[well_id,[f'Spike Alert on minutes in Past {time_window}ay/s']] = peak_label_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Spike Alert on minutes in Past Week']] = peak_label_df.loc[pd.to_datetime(date)-pd.Timedelta('7d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Spike Alert on minutes in Past 30 Days']] = peak_label_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Spike Alert on minutes in Past 60 Days']] = peak_label_df.loc[start_date:pd.to_datetime(date)].sum().values[0]\n",
    "\n",
    "                todays_results_df.loc[well_id,[f'Ramp Alert on minutes in Past {time_window}ay/s']] = ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Ramp Alert on minutes in Past Week']] = ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta('7d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Ramp Alert on minutes in Past 30 Days']] = ramp_label_df.loc[pd.to_datetime(date)-pd.Timedelta('30d'):pd.to_datetime(date)].sum().values[0]\n",
    "                todays_results_df.loc[well_id,['Ramp Alert on minutes in Past 60 Days']] = ramp_label_df.loc[start_date:pd.to_datetime(date)].sum().values[0]\n",
    "\n",
    "                todays_results_df.loc[well_id,['Average gas flow (60 Days)']] = well_data_df.loc[start_date:date,[\"FLOW_GAS\"]].interpolate(\"ffill\").mean().values[0]\n",
    "                todays_results_df.loc[well_id,['TD status']] = completion_turndown_df[completion_turndown_df['WellCD']==well_id]['Turndown Cat.'].values[0]\n",
    "                todays_results_df.loc[well_id,['Design']] = completion_turndown_df[completion_turndown_df['WellCD']==well_id]['Completion Design'].values[0]\n",
    "                todays_results_df.loc[well_id,['Pump age']] = pd.to_datetime(date)-label_data_df[(label_data_df['WellCD']==well_id) & (label_data_df.index < date)&(label_data_df['Event']=='Pump Change')].index.max()\n",
    "                todays_results_df.loc[well_id,['Days since last flush']] = pd.to_datetime(date)-label_data_df[(label_data_df['WellCD']==well_id) & (label_data_df.index < date)&((label_data_df['Event']=='Scheduled')|(label_data_df['Event']=='Reactive')|(label_data_df['Event']=='Flushby'))].index.max()\n",
    "                todays_results_df.loc[well_id,['Days since last failure']] = pd.to_datetime(date)-label_data_df[(label_data_df['WellCD']==well_id) & (label_data_df.index < date) & (label_data_df['Failure Mode'].notnull())].index.max()       \n",
    "\n",
    "                todays_results_df.loc[well_id,[f'Spike Alert Percent Active Time in Past {time_window}ay/s']]=(todays_results_df.loc[well_id,[f'Spike Alert on minutes in Past {time_window}ay/s']].values[0]/(pd.Timedelta(time_window).total_seconds() / 60))*100\n",
    "                todays_results_df.loc[well_id,['Spike Alert Percent Active Time in Past Week']]=(todays_results_df.loc[well_id,['Spike Alert on minutes in Past Week']].values[0] / 10080)*100\n",
    "                todays_results_df.loc[well_id,['Spike Alert Percent Active Time in Past 30 Days']]=(todays_results_df.loc[well_id,['Spike Alert on minutes in Past 30 Days']].values[0]/43200)*100\n",
    "                todays_results_df.loc[well_id,['Spike Alert Percent Active Time in Past 60 Days']]=(todays_results_df.loc[well_id,['Spike Alert on minutes in Past 60 Days']].values[0]/86400)*100\n",
    "                                      \n",
    "                todays_results_df.loc[well_id,[f'Ramp Alert Percent Active Time in Past {time_window}ay/s']]=(todays_results_df.loc[well_id,[f'Ramp Alert on minutes in Past {time_window}ay/s']].values[0]/(pd.Timedelta(time_window).total_seconds() / 60))*100\n",
    "                todays_results_df.loc[well_id,['Ramp Alert Percent Active Time in Past Week']]=(todays_results_df.loc[well_id,['Ramp Alert on minutes in Past Week']].values[0] / 10080)*100\n",
    "                todays_results_df.loc[well_id,['Ramp Alert Percent Active Time in Past 30 Days']]=(todays_results_df.loc[well_id,['Ramp Alert on minutes in Past 30 Days']].values[0]/43200)*100\n",
    "                todays_results_df.loc[well_id,['Ramp Alert Percent Active Time in Past 60 Days']]=(todays_results_df.loc[well_id,['Ramp Alert on minutes in Past 60 Days']].values[0]/86400)*100\n",
    "                \n",
    "                \n",
    "                if plot:\n",
    "                    _ = get_static_plot_for_single_alert(well_id,\n",
    "                                                     well_data_df.loc[start_date:date],\n",
    "                                                     features_to_plot=[torque_col,\n",
    "                                                                       'SPEED_ROD',\n",
    "                                                                       'FLOW_GAS'\n",
    "                                                                      ],\n",
    "                                                     alert_names=['combined_alert','ramping_alert'],\n",
    "                                                     label_data_df=label_data_df[label_data_df['WellCD']==well_id].loc[(label_data_df[label_data_df['WellCD']==well_id].index > start_date) & (label_data_df[label_data_df['WellCD']==well_id].index < end_date)],\n",
    "                #                                      plot_type='line',\n",
    "                #                                      ylim=[200, 800],\n",
    "                #                                          save_fig_fname=f\"/home/ec2-user/SageMaker/efs/plots_bs/seg_inference/v2/{well_id}_{start_date}_{end_date}.png\"\n",
    "                                                    )\n",
    "\n",
    "    todays_results_df=todays_results_df.loc[pd.DataFrame(all_well_total_alert_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum())[all_well_total_alert_df.loc[pd.to_datetime(date)-pd.Timedelta(time_window):pd.to_datetime(date)].sum()>0].index]\n",
    "    return todays_results_df.sort_values(by=f'Total Alert on minutes in Past {time_window}ay/s',ascending=False)#, peak_label_df, ramp_label_df,total_alert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfa41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import glob\n",
    "date=pd.to_datetime('2020-05-03 23:59:00')#  23:59:00\n",
    "time_window='1d'\n",
    "file_list=glob.glob(f'/home/ec2-user/SageMaker/efs/labels/segnet_inference/BRETT_Multi_Well_Model_v2/*.pkl')\n",
    "well_list=[well.split('/')[-1].split('.')[0].split('_')[-1] for well in file_list]\n",
    "\n",
    "# well_list=[ 'RM07-17-1']#'RM03-75-1',\n",
    "#  'RM50-127-1',\n",
    "#  'RM09-15-1',\n",
    "#  'RM03-39-1']\n",
    "#     'RM08-19-4',\n",
    "#             'RM09-26-5',\n",
    "#             'RM08-19-2',\n",
    "#             'RM12-10-4',\n",
    "#             'RM09-18-1',\n",
    "#             'RM07-22-2'\n",
    "# ]\n",
    "torque_names = ['TORQUE_MOTOR', 'TORQUE_ROD']\n",
    "completion_turndown_df=pd.read_csv(f'/home/ec2-user/SageMaker/efs/data/All_Roma_Completion_Turndown_ProductionStatus.csv')\n",
    "label_data_df = pd.read_csv(f\"/home/ec2-user/SageMaker/efs/data/All_Roma_Flush_Fail_PCPChange.csv\")\n",
    "label_data_df['Event Date'] = pd.to_datetime(label_data_df['Event Date'], format=\"%d/%m/%Y\")\n",
    "label_data_df.dropna(subset=['Event Date'], inplace=True)\n",
    "label_data_df.set_index('Event Date', inplace=True)\n",
    "# all_well_today_df, peak_label_df, ramp_label_df, total_alert_df=alert_monitor_generation_all_wells(date,\n",
    "all_well_today_df=alert_monitor_generation_all_wells(date,\n",
    "                                       time_window,\n",
    "                                      well_list,\n",
    "                                      1,\n",
    "                                      torque_names, completion_turndown_df,label_data_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d01286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f6ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operation_status(well_data_df: pd.DataFrame,\n",
    "                         feature_name: str,\n",
    "                         config_dict: dict):\n",
    "    \"\"\"\n",
    "    get_off_times gets...\n",
    "\n",
    "    :param well_data_df: dataframe containing all the data for a single well over which inference will be executed.\n",
    "    :param config_dict: a dictionary of model specific configuration (e.g. thresholds, etc.)\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "\n",
    "    off_trace = pd.DataFrame(index=well_data_df.index, columns=[config_dict['operation_method']])\n",
    "    off_trace[config_dict['operation_method']] = 0\n",
    "    torque_well_data_df = well_data_df[feature_name].interpolate(method='linear', axis=0)\n",
    "    off_times = torque_well_data_df[torque_well_data_df < config_dict['off_threshold']].index\n",
    "    prev_point = well_data_df.index[0] - pd.Timedelta('6H')\n",
    "    for point in off_times:\n",
    "        if point > (prev_point + pd.Timedelta('5H')):\n",
    "            off_trace[config_dict['operation_method']].loc[point - pd.Timedelta('62min'):point + pd.Timedelta('5H')] = 1\n",
    "            prev_point = point\n",
    "    well_data_df[config_dict['operation_method']] = off_trace\n",
    "\n",
    "    return well_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_disk(data_definition_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw data from CSV file\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    well_data_df = pd.read_csv(data_definition_dict['well_raw_data_path'])\n",
    "    if 'Unnamed: 0' in well_data_df.columns:\n",
    "        well_data_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "    well_data_df['TS'] = pd.to_datetime(well_data_df['TS'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    well_data_df.set_index(['TS'], inplace=True, drop=True)  \n",
    "    well_data_df = well_data_df.loc[(well_data_df.index > data_definition_dict['start_date']) & (well_data_df.index < data_definition_dict['end_date'])]\n",
    "    return well_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ec2-user/SageMaker/efs/data/october_data_extract/tag_RM07-17-1.csv')\n",
    "df['TS'] = pd.to_datetime(df['TS'], format='%Y-%m-%d %H:%M:%S')\n",
    "df.set_index(['TS'], inplace=True, drop=True)\n",
    "df = df.loc[\"2020-02-03 23:59:00\":\"2020-05-03 23:59:00\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633de9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_operation_status(df, 'TORQUE_MOTOR', {'operation_method': 'operation_status','off_threshold': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ad7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list=glob.glob(f'/home/ec2-user/SageMaker/efs/labels/segnet_inference/BRETT_Multi_Well_Model_v2/*.pkl')\n",
    "well_list=[well.split('/')[-1].split('.')[0].split('_')[-1] for well in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be4e63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "well_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0059ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_to_1min(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05a46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8e82fe965d67b4d52727d9e01393d93b5bd7738491028c5420b39b71b9499d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
